---
title: "DM_final_project"
author: "Cindy Zhang, Tingting Gu, Partha Chatterjee, Roshan Kumar, Namratha Vishwanath, Akash"
date: "12/11/2018"
output: html_notebook
---


#Don't Get Kicked!
##Introduction

In this report, we analyze a dataset related to vehicle purchase and predict the risk of a car being a "lemon" or "kick", which are unfortunate buys of cars with serious issues. It's very common for used-car buyers to end up with a problematic vehicle, due to one-sided arbitration clause and information asymmetry. These bad-buys are costly or sometimes impossible to fix, troubling dealers who acquire cars at auctions as now they cannot sell the cars to customers. Dishonest dealers may disclose the car history untruthfully, misreprenting the facts in order to sell cars to their customers.

We attempt to address the lemon car problem by modeling and predicting which cars have a higher risk of being a lemon using several Classficiation models. We are interested in the model with the highest accuracy and sensitivity. Our goal is to help uninformed buyers identify kicked cars and make better decisions.

The dataset is provided by a past competition from Carvana: https://www.kaggle.com/c/DontGetKicked/data. 

#1. Basic Data Understanding

## 1.1 Data Description
The competition dataset contains a training dataset of 72,983 observations with 34 variables. The test dataset contains 48,707 obversations. 

The dependent variable IsBadBuy indicates whether a vehicle has the condition of "kicked" or not, which is labeled as "1" or "0." The test dataset does not have IsBadBuy the dependent variable. Therefore, we will need to create a training and test datasets using the Training dataset to validate the performance of our models. 

According to the "Carvana_Data_Dictionary.txt" on Kaggle, the data includes RefID, which is an sequential number assigned to each vehicle. Another ID is BYRNO, which is a number assigned to the buyer of the vehicle. 

A few of the variables are related to the vehicles. VehicleAgeï¼Œ VehYear and PurchDate represent the age of the cars. Make, Model, Trim, Submodel describe the model information of the cars. WheelTypeID and WheelType both describe the wheels of the cars. Color, Transmission, VehOdo (how many miles the cars have), Natinoality, TopThreeAmericanName, Size are as the variable names suggest. TopThreeAmericanName identifies if the car manufacturer is one of the top 3 American brands.

The dataset also includes a few variables that describe the price of cars: 
MMRAcquisitionAuctionAveragePrice: Acquisition price for this vehicle in average condition at time of purchase	
MMRAcquisitionAuctionCleanPrice: Acquisition price for this vehicle in the above Average condition at time of purchase
MMRAcquisitionRetailAveragePrice: Acquisition price for this vehicle in the retail market in average condition at time of purchase
MMRAcquisitonRetailCleanPrice: Acquisition price for this vehicle in the retail market in above average condition at time of purchase
MMRCurrentAuctionAveragePrice: Acquisition price for this vehicle in average condition as of current day	
MMRCurrentAuctionCleanPrice: Acquisition price for this vehicle in the above condition as of current day
MMRCurrentRetailAveragePrice: Acquisition price for this vehicle in the retail market in average condition as of current day
MMRCurrentRetailCleanPrice: Acquisition price for this vehicle in the retail market in above average condition as of current day

We can see from the chart below that AUCGUART and PRIMEUNIT has more than 95% missing data. PRIMEUNIT identifies if the car would have a higher demand than a standard unit. AUCGUART indicates the level of guarantee provided at the auction. 

VNST and VNZIP1 indicate the the state and location where the car was purchased. 


```{r EDA Basic}
library(ggplot2)
library(DataExplorer)
library(dplyr)
library(recipes)
library(randomForest)
library(caret)
library(pROC)

ds <- as.data.frame(read.csv("Data/training/training.csv", na.strings = c("NULL","","NOT AVAIL%","NA%")))
plot_intro(ds)
sapply(ds,class)
plot_missing(ds)

```

## 1.2 Basic Data Transformation

With the initial exploration of data, we performed some basic data transformation to the data set. 

###Set Categorical Variables:
The dependent feature IsBadBuy is set to be a categorical feature of values "Yes"/"No." Similarly, VehYear and IsOnlineSale, which means if the sale happened online are also set to be categorical.

###Handling Missing Values
We dropped AUCGUARD and PRIMEUNIT fields since less than 4% of observations are available. There is no meaningful way we could impute this information. 


```{r Data Transformation Basic}
#converting data type 
ds$IsBadBuy <-as.factor(ifelse(ds$IsBadBuy==1,"Yes","NO"))
ds$VehYear <- as.factor(ds$VehYear)
ds$IsOnlineSale <-as.factor(ds$IsOnlineSale)

#Extracting Year/Month from purchdate
ds$purch_year <- format(as.Date(ds$PurchDate, "%m/%d/%Y"), "%Y") %>% as.factor()
ds$purch_month <- format(as.Date(ds$PurchDate, "%m/%d/%Y"), "%m") %>% as.factor()

#Dropping Columns with missing values 
ds <- drop_columns(ds, c("AUCGUART","PRIMEUNIT")) 

```

## 1.3 Exploration of the Relaitonship between Variables

Next, we plotted some graphs to understand the relationship between variables. The data is an imbalanced dataset, with a lot more negative observations than positive observations. We will need to use downsampling/oversampling in our model to correct this.

```{r EDA exploration on the variables }
ggplot(ds, aes(x=IsBadBuy, fill=IsBadBuy)) +geom_bar() 

dis_m <- sapply(ds, is.factor)
dis_name <- names(dis_m)[dis_m]
ds_dis <- ds[dis_m]

cont_m <- vapply(ds, function(x) is.numeric(x) & length(unique(x))>2, logical(1))
cont_name <- names(cont_m)[cont_m]
ds_cont <- ds[cont_m]    

```

```{r EDA graph for continuous variables}

plot_density(ds[c(cont_name)])


#basic plots on IsBadBuy against continuous variables 
ggplot(ds, aes(x = ds$IsBadBuy, y=VehicleAge, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="VehicleAge")

ggplot(ds, aes(x = ds$IsBadBuy, y=ds$MMRAcquisitionRetailAveragePrice, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="MMRAcquisitionRetailAveragePrice")

ggplot(ds, aes(x = ds$IsBadBuy, y=ds$MMRAcquisitionRetailAveragePrice, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="MMRAcquisitionRetailCleanPrice")

ggplot(ds, aes(x = ds$IsBadBuy, y=ds$VehOdo, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="VehOdo")

ggplot(ds, aes(x = ds$IsBadBuy, y=ds$VehBCost, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="VehBCost")

ggplot(ds, aes(x = ds$IsBadBuy, y=ds$VNZIP1, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="VNZIP1")


ggplot(ds, aes(x = ds$IsBadBuy, y=ds$MMRAcquisitionAuctionAveragePrice, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="AcquisitionAuctionAveragePrice")

ggplot(ds, aes(x = ds$IsBadBuy, y=ds$MMRCurrentRetailAveragePrice, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="MMRCurrentRetailAveragePrice")

ggplot(ds, aes(x = ds$IsBadBuy, y=ds$WarrantyCost, fill=ds$IsBadBuy)) + 
                                geom_boxplot() +
                                labs(x= "lemon",y="WarrantyCost")


#more advanced plots to uncover the six price attributes and their implications on the prediction

ggplot(data = ds, aes(x = MMRAcquisitionAuctionAveragePrice , y = MMRAcquisitionAuctionCleanPrice)) + 
  geom_point(mapping = aes(color = IsBadBuy)) + 
  geom_smooth()

ggplot(data = ds, aes(x = MMRAcquisitionRetailAveragePrice, y = MMRAcquisitonRetailCleanPrice)) + 
  geom_point(mapping = aes(color = IsBadBuy)) + 
  geom_smooth()

ggplot(data = ds, aes(x = MMRCurrentAuctionAveragePrice, y = MMRCurrentAuctionCleanPrice)) + 
  geom_point(mapping = aes(color = IsBadBuy)) + 
  geom_smooth()


ggplot(data = ds, aes(x = MMRCurrentRetailAveragePrice, y = MMRCurrentRetailCleanPrice)) + 
  geom_point(mapping = aes(color = IsBadBuy)) + 
  geom_smooth()


ggplot(data = ds, aes(x = MMRCurrentRetailAveragePrice, y = MMRAcquisitionRetailAveragePrice)) + 
  geom_point(mapping = aes(color = IsBadBuy)) + 
  geom_smooth()

ggplot(data = ds, aes(x = MMRCurrentAuctionAveragePrice, y = MMRAcquisitionAuctionAveragePrice)) + 
  geom_point(mapping = aes(color = IsBadBuy)) + 
  geom_smooth()

```


## 1.4 Advanced Data Transformation

###Remove Variables
After extrating year and month, we dropped PurchDate field. 
Based on our exploration above, we decided to drop the RefId and BYRNO since they do not indicate to the condition of the car. We also dropped WheelTypeID since it overlaps with WheelType. We dropped VNZIP1 since it overlaps with VNST information. 

###Feature Extraction and Reengineering
For Transmission, we collapse the capitalized MANUAL with lowercase manual

After looking at the number of levels in SubModel in reference to other features, we decided to create a new column SubmodelType with information extracted from SubModel. This new feature will provide basic vehile type with little overlapping information with other existing features.


###Handling Missing Values
For Discrete and Categorical fields, we set the missing values to be the most frequent value: 
- TopThreeAmericanName: 5 NAs set to GM
- Size: 5 NAs set to Compact
- Nationality: 5 NAs set to American
- Color: 8 NAs set to Silver
- SubModel: 8 NAs set to Other
- Transmission: 9 NAs set to AUTO
- WheelType: 3,714 NAs set to Alloy
- Trim: 2,360 NAs set to Bas

For Continuous fields, we set the missing value to be the mean of the variable (without the missing observations). The continueous fields that have missing values are the MMR...PRICE fields.


```{r Data Transformation dropping irrelevant columns}
#dropping less inforamtive columns
ds <- drop_columns(ds, c("RefId","VehicleAge","BYRNO", "WheelTypeID","VNZIP1","PurchDate"))

```

```{r EDA graph for discrete variables}
plot_bar(ds_dis)
```

```{r Data Transformation Feature Reengineering }
#Combining the same category into one 
unique(ds$Transmission)
ds$Transmission[grep("Manual", ds$Transmission , ignore.case=FALSE, fixed=FALSE)] <- "MANUAL"
ds$Transmission <- factor(ds$Transmission, levels=c("AUTO","MANUAL"))

#extracting new feature SubModelType
ds$SubModelType <- NA
ds$SubModelType[grep("CAB", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "CAB"
ds$SubModelType[grep("CUV", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "CUV"
ds$SubModelType[grep("MINIVAN", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "MINIVAN"
ds$SubModelType[grep("UTILITY", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "UTILITY"
ds$SubModelType[grep("SPORT", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "SPORT"
ds$SubModelType[grep("PASSENGER", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "PASSENGER"
ds$SubModelType[grep("SUV", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "SUV"
ds$SubModelType[grep("WAGON", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "WAGON"
ds$SubModelType[grep("CONVERTIBLE", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "CONVERTIBLE"
ds$SubModelType[grep("HATCHBACK", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "HATCHBACK"
ds$SubModelType[grep("COUPE", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "COUPE"
ds$SubModelType[grep("SEDAN", ds$SubModel , ignore.case=TRUE, fixed=FALSE)] <- "SEDAN"
ds$SubModel <- NULL

```

```{r Data Transformation Missing Values Imputation }
#Imputing Missing Values for Discrete Using Most Frequent Value
i1 <- !sapply(ds,is.numeric)

Mode <- function(x) { 
      ux <- sort(unique(x))
      ux[which.max(tabulate(match(x, ux)))] 
}
ds[i1] <- lapply(ds[i1], function(x)
              replace(x, is.na(x), Mode(x[!is.na(x)])))
#Imputing Missing Values for Continuous Using Mean
for (i in which(sapply(ds, is.numeric))) {
    ds[is.na(ds[, i]), i] <- mean(ds[, i], na.rm = TRUE)
}

```


#2. Model Fitting 

##2.1  Model Introduction
We have chosen five models for this datasets. Random Forest (Ranger), Cart Model (Rpart2), Tree Bag Model (Treebag), Boosting Model(adaboos), Logistic Regression (Glmnet) and Nueral Network Model (Nnet)

###Random Forest (Ranger)
Random Forest or rndom forest decision tree is one of the commonly used ensembling learning method for classification problems. It is a supervised lernng method that uses bagging to train the training dataset.
In this model we are using 10-fold cross validation as our resampling technique. The splitting rule used is based on the gini index. We have also selected the minimum node size as 3. The sampling parameter is up.

###Cart Model (Rpart2) 
Cart Model is also a commonly used ensembling technique for classification problems. This method also uses bagging. The resampling technique is same as Random forest. The max.depth tuning parameter chosen for this parameter is between 1 and 20. The sampling parameter is up.

###Tree Bag Model (Treebag)
Treebag model is an enhancement of the decision tree ensembling technique. The tuning parameters are same as that of cart model.

###Boosting model(adaboos):
Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers. 
This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added. AdaBoost is best used to boost the performance of decision trees on binary classification problems. The tunelength is set to 2 and the max.depth is set to 1 and 3

###Logistic regression(glmNet)
In this method, input values (x) are combined linearly using weights or coefficient value to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a numeric value. ROC is used as a metric of model measure. Alpha is set from 0 to 1 and lambda from 0.0001 to 1 with a length of 20


###Neural Network (nnet)
This method sets the default hidden layer to one, there are two parameters we are tuning. The size which records the number of units in the hidden layer and decay is the regularization parameter to avoid overfitting. In our case, we set our grid with size in (5,6,7) and decay in (0.5,0.1), which is researched to be the most optimized parameters


##2.2 Sampling Methods for Class Imbalance 
Our prediction contains a class imbalance. As a result, we decided to adopt both downsampling and upsampling to mitigate its impact. We used upsampling for cart model and treebag model, while we used downsampling for random forest, boosting, logistic regression and neural net 

```{r Splitting Data into Test and Train }

set.seed(4595)

data_split <- initial_split(ds)
train_data <- training(data_split)
test_data  <- testing(data_split)
```


```{r Setting Down Sample Control}
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  savePredictions = "final",
  verboseIter = TRUE,
  summaryFunction = twoClassSummary,
  sampling = "down"
  )

```


```{r Bagging Recipe and Grid Setup}
bagging_rec <- recipe (IsBadBuy ~., data=train_data) %>%
  step_nzv(all_predictors())


rf_grid <- expand.grid(mtry = seq(2, 20, 2),
                      splitrule = "gini",
                      min.node.size = 3)

```


```{r RF Model Fitting}
rf_mod <- train(bagging_rec,
                data = train_data,
                method = "ranger",
                trControl = ctrl,
                metric = "ROC",
                tuneGrid = rf_grid,
                importance = "impurity")

```

```{r ConfusionMatrix for RF}
confusionMatrix(rf_mod)
ggplot(rf_mod) + theme(legend.position = "top")
rf_mod$finalModel

```

```{r ROC for RF}
plot_roc <- function(x, ...) {
  roc_obj <- roc(
    response = x[["obs"]], 
    predictor = x[["Yes"]], 
    levels = rev(levels(x$obs))
  )
  plot(roc_obj, ...)
}

plot_roc(rf_mod$pred)
```


```{r RF Variable Importance}

rf_imp <- varImp(rf_mod, scale = FALSE, 
                   surrogates = FALSE, 
                   competes = FALSE)

ggplot(rf_imp, top = 7) + xlab("")
```


```{r Test Set RF }
pred_rf <- predict(rf_mod, newdata=test_data)
confusionMatrix(pred_rf, test_data$IsBadBuy)
```


```{r Setting Up Sample Control}
cart_ctrl <- trainControl(
    method = "cv",
    classProbs = TRUE,
    summaryFunction = twoClassSummary,
    savePredictions = "final",
    sampling = "up",
    verboseIter = TRUE
    
)


```

```{r Cart Model Fitting}
cart_mod <- train(
    bagging_rec,
    data = train_data,
    method = "rpart2",
    metric = "ROC",
    tuneGrid = data.frame(maxdepth = 1:20),
    trControl = cart_ctrl
)

```


```{r ROC Curve for Cart Model}
plot_roc(cart_mod$pred)
```


```{r Cart Model Variable Importance}

cart_imp <- varImp(cart_mod, scale = FALSE, 
                   surrogates = FALSE, 
                   competes = FALSE)

ggplot(cart_imp, top = 7) + xlab("")

```

```{r Test Set ConfusionMatrix}
pred_cart_mod <- predict(cart_mod, test_data)
confusionMatrix(pred_cart_mod, test_data$IsBadBuy)
```


```{r Tree_bag Model Fitting}
tree_bag_mod <- train(
    bagging_rec,
    data = train_data,
    method = "treebag",
    metric = "ROC",
    trControl = cart_ctrl
)

```

```{r ConfusionMatrix for Tree_Bag}
confusionMatrix(tree_bag_mod)
tree_bag_mod
ggplot(tree_bag_mod) + theme(legend.position = "top")
tree_bag_mod$finalModel

```



```{r Test Data ConfusionMatrix}
pred_tree_bag <- predict(tree_bag_mod, test_data)
confusionMatrix(pred_tree_bag, test_data$IsBadBuy)
```


```{r Boostong Recipe and Grid Prep}
boosting_rec <- recipe (IsBadBuy ~., data=train_data) %>%
  step_nzv(all_predictors())

ada_grid <-  expand.grid(mfinal = (1:3)*3, maxdepth = c(1, 3),
                    coeflearn = c("Breiman", "Freund", "Zhu"))

```

```{r Boosting Model Fitting}
model_adaboost = train(boosting_rec, data=train_data, method='AdaBoost.M1', tuneLength=2, trControl = ctrl,tuneGrid = ada_grid)

```

```{r}
confusionMatrix(model_adaboost)

```


```{r Test Set ConfusionMatrix for Boosting}
pred_adaboost_mod <- predict(model_adaboost, test_data)
confusionMatrix(pred_adaboost_mod , test_data$IsBadBuy)
```



```{r GLMNET Model Recipe and Grid Prep}

glmnet_rec <- recipe(IsBadBuy ~ ., data = train_data) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(), -IsBadBuy)

glmnet_grid <- expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 20))

```


```{r GLMNET Model Fitting}
set.seed(3544)
glmn_mod <- train(
  glmnet_rec, 
  data = train_data,
  method = "glmnet", 
  metric = "ROC",
  trControl = ctrl,
  tuneGrid = glmnet_grid)

```
 
 
```{r ConfusionMatrix for GLMNET}
confusionMatrix(glmn_mod)
```
 
 
```{r Test Set ConfusionMatrix for GLMNET}
predict_glmn <- predict(glmn_mod, newdata =test_data)
confusionMatrix(predict_glmn, test_data$IsBadBuy)
```


```{r Neural Networks Model Recipe and Grid Prep}
nn_rec <- recipe (IsBadBuy ~ ., data = train_data) %>%
  step_nzv(all_predictors())

nn_grid <- expand.grid(.decay = c(0.5, 0.1), .size = c(5, 6, 7))

```

```{r Neural Networks Model Training}
nn_mod <- train(
               nn_rec,
               data=train_data, 
               trControl=ctrl,
               metric = "ROC",
               method="nnet", 
               MaxNWts=84581,
               tuneGrid = nn_grid)

```

```{r ConfusionMatrix for Neural Networks}
confusionMatrix(nn_mod)
```

```{r Test Set ConfusionMatrix for Neural Networks}

predict_nn <- predict(nn_mod, newdata =test_data)
confusionMatrix(predict_nn, test_data$IsBadBuy)

```


#3. Model Selection (Default Positive is "No")

##3.1 Final Model Prediction Results

We have chosen the Random Forest model based on the test set performance, which has an accuracy rate of 0.6286, a specificity rate of 0.6662 and a sensitivity rate of 0.6233. Even though the Neural Network model has a higher accuracy at 0.67, since we hope to obtain a higher accuracy rate in predicting "Yes" in our IsBadBuy column, we chose the model with the highest specificity, which portrays the probability of actual negatives that are correctly identified as such. The rationale behind our decision is that we want to focus on predicting and avoiding a lemon car/kick. We are less concerned if non-kick cars that are incorrectly classified as lemons.

Note that in our model, a lemon car is classified as "Yes" in the IsBadBuy column. The default positive is "No" in the IsBadBuy column. 

##3.2 Comments on Final Results

Our prediction results using test data outperformed training data validations across all 6 models. Further research showed that our models may be underfitting on our training data, which attributes to our sampling methods. Both upsampling and downsampling run the risks of compromising data integrity and rendering underfitting.  

Many factors contributed to the variations of our model performances. First, data splitting directly affects the performance results. We are likely to obtain different modeling results from another data split. Second, our cross-validation is set to 10 folds, which may not be sufficient to achieve adequate precision. Given the two factors, we evaluated and selected our final model based on test set performance.

```{r}
suppressMessages(library(tidyposterior))

rs <- resamples(
  list(RF=rf_mod, TREEBAG=tree_bag_mod, CART=cart_mod, GLMN=glmn_mod,NN=nn_mod, BOOST=model_adaboost)
)

roc_mod <- perf_mod(rs, seed = 2560, iter = 1000, metric = "ROC")
summary(broom::tidy(roc_mod))
```

```{r}
plot_roc(rf_mod$pred, col = "red")
plot_roc(tree_bag_mod$pred, col = "green", add = TRUE)
plot_roc(cart_mod$pred, col = "blue", add = TRUE)
plot_roc(glmn_mod$pred, col = "yellow", add = TRUE)
plot_roc(nn_mod$pred, col = "dark blue", add = TRUE)
plot_roc(model_adaboost$pred, col = "black", add = TRUE)

```



